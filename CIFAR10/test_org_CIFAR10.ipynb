{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish data loading\n",
      "Training data size: 5000\n",
      "Testing data size: 5000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.models import resnet18\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "\n",
    "# 指定需要的类别\n",
    "selected_classes = [0, 1, 2, 3, 4]\n",
    "label_mapping = {orig_label: idx for idx, orig_label in enumerate(selected_classes)}\n",
    "# 获取 CIFAR-100 的类别映射\n",
    "cifar100_classes = torchvision.datasets.CIFAR10(root='./data', download=False).classes\n",
    "\n",
    "# 数据预处理\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# 加载 CIFAR-100 数据集\n",
    "train_set = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=False, transform=transform_train\n",
    ")\n",
    "\n",
    "# 筛选出所需类别的索引\n",
    "selected_indices_train_class = [\n",
    "    idx for idx, (_, label) in enumerate(train_set)\n",
    "    if label in selected_classes\n",
    "]\n",
    "random.shuffle(selected_indices_train_class)\n",
    "selected_indices_train=[]\n",
    "class_cnt=[0]*10\n",
    "for i in selected_indices_train_class:\n",
    "    if class_cnt[train_set.targets[i]]<1000:\n",
    "        selected_indices_train.append(i)\n",
    "        class_cnt[train_set.targets[i]]+=1\n",
    "\n",
    "for i in selected_indices_train:\n",
    "    train_set.targets[i]=label_mapping[train_set.targets[i]]\n",
    "\n",
    "\n",
    "bt_size=128\n",
    "# 创建子集数据集\n",
    "filtered_train_set = Subset(train_set, selected_indices_train)\n",
    "\n",
    "train_loader = DataLoader(filtered_train_set, batch_size=bt_size, shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 加载 CIFAR-100 测试集\n",
    "test_set = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=False, transform=transform_test\n",
    ")\n",
    "\n",
    "\n",
    "selected_indices_test = [\n",
    "    idx for idx, (_, label) in enumerate(test_set)\n",
    "    if label in selected_classes\n",
    "]\n",
    "\n",
    "\n",
    "for i in selected_indices_test:\n",
    "    test_set.targets[i]=label_mapping[test_set.targets[i]]\n",
    "\n",
    "# 创建子集数据集\n",
    "filtered_test_set = Subset(test_set, selected_indices_test)\n",
    "test_loader = DataLoader(filtered_test_set, batch_size=bt_size, shuffle=False, num_workers=2)\n",
    "\n",
    "print('Finish data loading')\n",
    "print(f\"Training data size: {len(filtered_train_set)}\")\n",
    "print(f\"Testing data size: {len(filtered_test_set)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" def train(model, train_loader, epochs=10, device='cpu'):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "    criterion = torch.nn.CrossEntropyLoss()# give y hat and y calculate the loss\n",
    "    for i in range(epochs):\n",
    "\n",
    "        loss_final = 0.0\n",
    "        loop = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "        loop.set_description(f'Epoch [{i}/{epochs}]')\n",
    "\n",
    "        for step, (images, labels) in loop:\n",
    "\n",
    "            \n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            # forward + backward + update\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_final += loss.item()\n",
    "            loop.set_postfix(loss=loss_final)\n",
    "        \n",
    "        test(test_loader,model,device) \"\"\"\n",
    "\n",
    "def test(test_loader,model,device='cpu'):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:#包含batch size=64张图片和label\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, dim=1)#predicted是一个tensor,torch.tensor([1, 2, 3, ..., 64])  # 共 64 个元素\n",
    "            # 返回最大值和下标\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print('Accuracy on test set: %.3f %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 512)\n",
    "        self.fc2 = nn.Linear(512, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 8 * 8)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/chunjie/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MobileNetV2(\n",
       "  (features): Sequential(\n",
       "    (0): ConvBNActivation(\n",
       "      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6(inplace=True)\n",
       "    )\n",
       "    (1): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
       "          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(8, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(48, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (3): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(96, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (4): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(96, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(96, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(96, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (8): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (9): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (10): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (11): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (12): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)\n",
       "          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (13): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)\n",
       "          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (14): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)\n",
       "          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(288, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (15): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (16): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (17): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(480, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (18): ConvBNActivation(\n",
       "      (0): Conv2d(160, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.2, inplace=False)\n",
       "    (1): Linear(in_features=1280, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = torch.hub.load(\"chenyaofo/pytorch-cifar-models\", \"cifar10_mobilenetv2_x0_5\", pretrained=False)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    running_loss = 0.0 \n",
    "\n",
    "    \n",
    "    for batch_idx, data in enumerate(train_loader, 0):\n",
    "\n",
    "        inputs, target = data\n",
    "        inputs, target = inputs.to(device) , target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        #print(batch_idx)\n",
    "\n",
    "        # forward + backward + update\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    print('epoch: %d loss:%.3f ' % (epoch,running_loss), end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss:123.236  Accuracy on test set: 24.440 %\n",
      "epoch: 1 loss:68.098  Accuracy on test set: 41.900 %\n",
      "epoch: 2 loss:55.631  Accuracy on test set: 43.860 %\n",
      "epoch: 3 loss:51.651  Accuracy on test set: 47.020 %\n",
      "epoch: 4 loss:49.976  Accuracy on test set: 51.860 %\n",
      "epoch: 5 loss:46.399  Accuracy on test set: 53.180 %\n",
      "epoch: 6 loss:44.895  Accuracy on test set: 55.260 %\n",
      "epoch: 7 loss:44.838  Accuracy on test set: 54.980 %\n",
      "epoch: 8 loss:42.859  Accuracy on test set: 56.840 %\n",
      "epoch: 9 loss:42.850  Accuracy on test set: 53.460 %\n",
      "epoch: 10 loss:43.927  Accuracy on test set: 56.340 %\n",
      "epoch: 11 loss:41.576  Accuracy on test set: 58.020 %\n",
      "epoch: 12 loss:39.476  Accuracy on test set: 60.140 %\n",
      "epoch: 13 loss:38.772  Accuracy on test set: 59.640 %\n",
      "epoch: 14 loss:39.396  Accuracy on test set: 60.040 %\n",
      "epoch: 15 loss:38.742  Accuracy on test set: 61.800 %\n",
      "epoch: 16 loss:37.884  Accuracy on test set: 61.880 %\n",
      "epoch: 17 loss:37.103  Accuracy on test set: 61.000 %\n",
      "epoch: 18 loss:36.193  Accuracy on test set: 64.080 %\n",
      "epoch: 19 loss:35.519  Accuracy on test set: 61.320 %\n",
      "epoch: 20 loss:37.677  Accuracy on test set: 63.320 %\n",
      "epoch: 21 loss:34.563  Accuracy on test set: 64.800 %\n",
      "epoch: 22 loss:34.942  Accuracy on test set: 65.040 %\n",
      "epoch: 23 loss:34.129  Accuracy on test set: 66.440 %\n",
      "epoch: 24 loss:32.758  Accuracy on test set: 65.500 %\n",
      "epoch: 25 loss:32.777  Accuracy on test set: 68.120 %\n",
      "epoch: 26 loss:29.740  Accuracy on test set: 66.720 %\n",
      "epoch: 27 loss:30.684  Accuracy on test set: 66.740 %\n",
      "epoch: 28 loss:30.131  Accuracy on test set: 67.840 %\n",
      "epoch: 29 loss:29.309  Accuracy on test set: 71.460 %\n",
      "epoch: 30 loss:27.375  Accuracy on test set: 68.140 %\n",
      "epoch: 31 loss:28.249  Accuracy on test set: 70.240 %\n",
      "epoch: 32 loss:28.675  Accuracy on test set: 70.340 %\n",
      "epoch: 33 loss:32.694  Accuracy on test set: 68.980 %\n",
      "epoch: 34 loss:30.720  Accuracy on test set: 69.120 %\n",
      "epoch: 35 loss:27.314  Accuracy on test set: 68.300 %\n",
      "epoch: 36 loss:34.029  Accuracy on test set: 69.280 %\n",
      "epoch: 37 loss:29.869  Accuracy on test set: 71.060 %\n",
      "epoch: 38 loss:27.752  Accuracy on test set: 71.820 %\n",
      "epoch: 39 loss:25.654  Accuracy on test set: 72.960 %\n",
      "epoch: 40 loss:22.670  Accuracy on test set: 73.540 %\n",
      "epoch: 41 loss:22.915  Accuracy on test set: 74.260 %\n",
      "epoch: 42 loss:22.669  Accuracy on test set: 74.000 %\n",
      "epoch: 43 loss:25.458  Accuracy on test set: 73.900 %\n",
      "epoch: 44 loss:24.086  Accuracy on test set: 74.600 %\n",
      "epoch: 45 loss:23.496  Accuracy on test set: 73.200 %\n",
      "epoch: 46 loss:24.736  Accuracy on test set: 75.600 %\n",
      "epoch: 47 loss:22.343  Accuracy on test set: 75.660 %\n",
      "epoch: 48 loss:19.759  Accuracy on test set: 74.020 %\n",
      "epoch: 49 loss:20.028  Accuracy on test set: 76.620 %\n",
      "epoch: 50 loss:19.210  Accuracy on test set: 76.960 %\n",
      "epoch: 51 loss:19.747  Accuracy on test set: 78.020 %\n",
      "epoch: 52 loss:21.314  Accuracy on test set: 75.800 %\n",
      "epoch: 53 loss:19.363  Accuracy on test set: 76.660 %\n",
      "epoch: 54 loss:18.827  Accuracy on test set: 77.480 %\n",
      "epoch: 55 loss:21.828  Accuracy on test set: 76.080 %\n",
      "epoch: 56 loss:16.942  Accuracy on test set: 78.260 %\n",
      "epoch: 57 loss:15.789  Accuracy on test set: 77.800 %\n",
      "epoch: 58 loss:16.022  Accuracy on test set: 78.460 %\n",
      "epoch: 59 loss:14.416  Accuracy on test set: 78.080 %\n",
      "epoch: 60 loss:14.985  Accuracy on test set: 78.480 %\n",
      "epoch: 61 loss:17.446  Accuracy on test set: 78.200 %\n",
      "epoch: 62 loss:17.429  Accuracy on test set: 77.720 %\n",
      "epoch: 63 loss:17.517  Accuracy on test set: 76.740 %\n",
      "epoch: 64 loss:18.116  Accuracy on test set: 77.900 %\n",
      "epoch: 65 loss:14.175  Accuracy on test set: 79.460 %\n",
      "epoch: 66 loss:12.758  Accuracy on test set: 78.080 %\n",
      "epoch: 67 loss:13.774  Accuracy on test set: 79.480 %\n",
      "epoch: 68 loss:11.761  Accuracy on test set: 78.900 %\n",
      "epoch: 69 loss:12.537  Accuracy on test set: 79.240 %\n",
      "epoch: 70 loss:13.657  Accuracy on test set: 79.000 %\n",
      "epoch: 71 loss:12.483  Accuracy on test set: 79.120 %\n",
      "epoch: 72 loss:12.640  Accuracy on test set: 79.080 %\n",
      "epoch: 73 loss:11.444  Accuracy on test set: 78.960 %\n",
      "epoch: 74 loss:11.747  Accuracy on test set: 79.720 %\n",
      "epoch: 75 loss:9.575  Accuracy on test set: 79.020 %\n",
      "epoch: 76 loss:9.387  Accuracy on test set: 79.380 %\n",
      "epoch: 77 loss:8.727  Accuracy on test set: 79.360 %\n",
      "epoch: 78 loss:9.338  Accuracy on test set: 79.540 %\n",
      "epoch: 79 loss:8.624  Accuracy on test set: 80.300 %\n",
      "epoch: 80 loss:10.057  Accuracy on test set: 79.900 %\n",
      "epoch: 81 loss:7.627  Accuracy on test set: 80.400 %\n",
      "epoch: 82 loss:7.435  Accuracy on test set: 80.240 %\n",
      "epoch: 83 loss:6.901  Accuracy on test set: 80.200 %\n",
      "epoch: 84 loss:6.419  Accuracy on test set: 80.420 %\n",
      "epoch: 85 loss:5.979  Accuracy on test set: 80.320 %\n",
      "epoch: 86 loss:6.185  Accuracy on test set: 80.220 %\n",
      "epoch: 87 loss:6.632  Accuracy on test set: 80.900 %\n",
      "epoch: 88 loss:5.949  Accuracy on test set: 80.440 %\n",
      "epoch: 89 loss:6.456  Accuracy on test set: 80.480 %\n",
      "epoch: 90 loss:5.131  Accuracy on test set: 80.200 %\n",
      "epoch: 91 loss:5.518  Accuracy on test set: 80.520 %\n",
      "epoch: 92 loss:5.122  Accuracy on test set: 80.800 %\n",
      "epoch: 93 loss:5.558  Accuracy on test set: 80.740 %\n",
      "epoch: 94 loss:4.834  Accuracy on test set: 80.600 %\n",
      "epoch: 95 loss:5.155  Accuracy on test set: 80.800 %\n",
      "epoch: 96 loss:4.672  Accuracy on test set: 80.800 %\n",
      "epoch: 97 loss:4.261  Accuracy on test set: 80.860 %\n",
      "epoch: 98 loss:4.294  Accuracy on test set: 80.740 %\n",
      "epoch: 99 loss:5.045  Accuracy on test set: 80.900 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    epoch=100\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "    criterion = torch.nn.CrossEntropyLoss()# give y hat and y calculate the loss\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)\n",
    "    for i in range(epoch):\n",
    "        train(i)\n",
    "        scheduler.step()\n",
    "        test(test_loader,model,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 81.380 %\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test(test_loader,model,device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at: ./model_CIFAR10/model_org.pt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "save_dir = './model_CIFAR10'  # 目录\n",
    "save_path = f'{save_dir}/model_org.pt'  # 文件路径\n",
    "\n",
    "# 确保目录存在，但不要创建文件\n",
    "os.makedirs(save_dir, exist_ok=True)  # ✅ 只创建目录，不影响文件\n",
    "\n",
    "# 保存模型\n",
    "torch.save(model.state_dict(), save_path)  # ✅ 保存文件\n",
    "print(f\"Model saved at: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/chunjie/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n",
      "/tmp/ipykernel_1023834/717139178.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_test.load_state_dict(torch.load(f'./model_CIFAR10/model_org.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 81.380 %\n",
      "Accuracy on test set: 92.420 %\n"
     ]
    }
   ],
   "source": [
    "model_test=torch.hub.load(\"chenyaofo/pytorch-cifar-models\", \"cifar10_mobilenetv2_x0_5\", pretrained=False)\n",
    "model_test.to(device)\n",
    "model_test.load_state_dict(torch.load(f'./model_CIFAR10/model_org.pt'))\n",
    "model_test.eval()\n",
    "test(test_loader,model_test,device)\n",
    "test(train_loader,model_test,device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
