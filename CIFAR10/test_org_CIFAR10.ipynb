{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish data loading\n",
      "Training data size: 5000\n",
      "Testing data size: 5000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.models import resnet18\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "\n",
    "# 指定需要的类别\n",
    "selected_classes = [0, 1, 2, 3, 4]\n",
    "label_mapping = {orig_label: idx for idx, orig_label in enumerate(selected_classes)}\n",
    "# 获取 CIFAR-100 的类别映射\n",
    "cifar100_classes = torchvision.datasets.CIFAR10(root='./data', download=False).classes\n",
    "\n",
    "# 数据预处理\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# 加载 CIFAR-100 数据集\n",
    "train_set = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=False, transform=transform_train\n",
    ")\n",
    "\n",
    "# 筛选出所需类别的索引\n",
    "selected_indices_train_class = [\n",
    "    idx for idx, (_, label) in enumerate(train_set)\n",
    "    if label in selected_classes\n",
    "]\n",
    "random.shuffle(selected_indices_train_class)\n",
    "selected_indices_train=[]\n",
    "class_cnt=[0]*10\n",
    "for i in selected_indices_train_class:\n",
    "    if class_cnt[train_set.targets[i]]<1000:\n",
    "        selected_indices_train.append(i)\n",
    "        class_cnt[train_set.targets[i]]+=1\n",
    "\n",
    "for i in selected_indices_train:\n",
    "    train_set.targets[i]=label_mapping[train_set.targets[i]]\n",
    "\n",
    "\n",
    "bt_size=128\n",
    "# 创建子集数据集\n",
    "filtered_train_set = Subset(train_set, selected_indices_train)\n",
    "\n",
    "train_loader = DataLoader(filtered_train_set, batch_size=bt_size, shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 加载 CIFAR-100 测试集\n",
    "test_set = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=False, transform=transform_test\n",
    ")\n",
    "\n",
    "\n",
    "selected_indices_test = [\n",
    "    idx for idx, (_, label) in enumerate(test_set)\n",
    "    if label in selected_classes\n",
    "]\n",
    "\n",
    "\n",
    "for i in selected_indices_test:\n",
    "    test_set.targets[i]=label_mapping[test_set.targets[i]]\n",
    "\n",
    "# 创建子集数据集\n",
    "filtered_test_set = Subset(test_set, selected_indices_test)\n",
    "test_loader = DataLoader(filtered_test_set, batch_size=bt_size, shuffle=False, num_workers=2)\n",
    "\n",
    "print('Finish data loading')\n",
    "print(f\"Training data size: {len(filtered_train_set)}\")\n",
    "print(f\"Testing data size: {len(filtered_test_set)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" def train(model, train_loader, epochs=10, device='cpu'):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "    criterion = torch.nn.CrossEntropyLoss()# give y hat and y calculate the loss\n",
    "    for i in range(epochs):\n",
    "\n",
    "        loss_final = 0.0\n",
    "        loop = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "        loop.set_description(f'Epoch [{i}/{epochs}]')\n",
    "\n",
    "        for step, (images, labels) in loop:\n",
    "\n",
    "            \n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            # forward + backward + update\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_final += loss.item()\n",
    "            loop.set_postfix(loss=loss_final)\n",
    "        \n",
    "        test(test_loader,model,device) \"\"\"\n",
    "\n",
    "def test(test_loader,model,device='cpu'):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:#包含batch size=64张图片和label\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, dim=1)#predicted是一个tensor,torch.tensor([1, 2, 3, ..., 64])  # 共 64 个元素\n",
    "            # 返回最大值和下标\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print('Accuracy on test set: %.3f %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 512)\n",
    "        self.fc2 = nn.Linear(512, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 8 * 8)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/chunjie/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MobileNetV2(\n",
       "  (features): Sequential(\n",
       "    (0): ConvBNActivation(\n",
       "      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6(inplace=True)\n",
       "    )\n",
       "    (1): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
       "          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(8, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(48, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (3): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(96, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (4): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(96, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(96, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(96, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (8): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (9): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (10): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (11): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (12): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)\n",
       "          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (13): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)\n",
       "          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (14): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)\n",
       "          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(288, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (15): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (16): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (17): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(480, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (18): ConvBNActivation(\n",
       "      (0): Conv2d(160, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.2, inplace=False)\n",
       "    (1): Linear(in_features=1280, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = torch.hub.load(\"chenyaofo/pytorch-cifar-models\", \"cifar10_mobilenetv2_x0_5\", pretrained=False)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    running_loss = 0.0 \n",
    "\n",
    "    \n",
    "    for batch_idx, data in enumerate(train_loader, 0):\n",
    "\n",
    "        inputs, target = data\n",
    "        inputs, target = inputs.to(device) , target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        #print(batch_idx)\n",
    "\n",
    "        # forward + backward + update\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    print('epoch: %d loss:%.3f ' % (epoch,running_loss), end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss:58.140  Accuracy on test set: 41.120 %\n",
      "epoch: 1 loss:57.101  Accuracy on test set: 45.960 %\n",
      "epoch: 2 loss:49.779  Accuracy on test set: 53.400 %\n",
      "epoch: 3 loss:45.518  Accuracy on test set: 53.960 %\n",
      "epoch: 4 loss:43.030  Accuracy on test set: 57.660 %\n",
      "epoch: 5 loss:42.266  Accuracy on test set: 56.980 %\n",
      "epoch: 6 loss:41.918  Accuracy on test set: 57.140 %\n",
      "epoch: 7 loss:42.232  Accuracy on test set: 55.560 %\n",
      "epoch: 8 loss:44.120  Accuracy on test set: 58.420 %\n",
      "epoch: 9 loss:40.978  Accuracy on test set: 57.740 %\n",
      "epoch: 10 loss:42.416  Accuracy on test set: 61.380 %\n",
      "epoch: 11 loss:37.644  Accuracy on test set: 57.100 %\n",
      "epoch: 12 loss:37.935  Accuracy on test set: 62.260 %\n",
      "epoch: 13 loss:38.233  Accuracy on test set: 65.380 %\n",
      "epoch: 14 loss:35.488  Accuracy on test set: 64.300 %\n",
      "epoch: 15 loss:35.721  Accuracy on test set: 66.000 %\n",
      "epoch: 16 loss:35.260  Accuracy on test set: 65.400 %\n",
      "epoch: 17 loss:34.970  Accuracy on test set: 63.620 %\n",
      "epoch: 18 loss:33.114  Accuracy on test set: 66.940 %\n",
      "epoch: 19 loss:32.650  Accuracy on test set: 68.200 %\n",
      "epoch: 20 loss:32.889  Accuracy on test set: 66.600 %\n",
      "epoch: 21 loss:32.358  Accuracy on test set: 68.560 %\n",
      "epoch: 22 loss:32.187  Accuracy on test set: 68.540 %\n",
      "epoch: 23 loss:30.562  Accuracy on test set: 66.460 %\n",
      "epoch: 24 loss:30.588  Accuracy on test set: 69.620 %\n",
      "epoch: 25 loss:28.850  Accuracy on test set: 69.200 %\n",
      "epoch: 26 loss:30.062  Accuracy on test set: 64.780 %\n",
      "epoch: 27 loss:32.646  Accuracy on test set: 70.240 %\n",
      "epoch: 28 loss:30.128  Accuracy on test set: 71.980 %\n",
      "epoch: 29 loss:27.554  Accuracy on test set: 71.860 %\n",
      "epoch: 30 loss:24.786  Accuracy on test set: 72.040 %\n",
      "epoch: 31 loss:25.130  Accuracy on test set: 72.220 %\n",
      "epoch: 32 loss:26.853  Accuracy on test set: 71.240 %\n",
      "epoch: 33 loss:25.490  Accuracy on test set: 72.720 %\n",
      "epoch: 34 loss:24.637  Accuracy on test set: 74.780 %\n",
      "epoch: 35 loss:25.611  Accuracy on test set: 74.140 %\n",
      "epoch: 36 loss:22.061  Accuracy on test set: 75.620 %\n",
      "epoch: 37 loss:21.335  Accuracy on test set: 73.640 %\n",
      "epoch: 38 loss:22.956  Accuracy on test set: 74.400 %\n",
      "epoch: 39 loss:23.311  Accuracy on test set: 75.380 %\n",
      "epoch: 40 loss:20.611  Accuracy on test set: 75.440 %\n",
      "epoch: 41 loss:22.571  Accuracy on test set: 76.160 %\n",
      "epoch: 42 loss:19.879  Accuracy on test set: 77.220 %\n",
      "epoch: 43 loss:19.522  Accuracy on test set: 77.340 %\n",
      "epoch: 44 loss:16.847  Accuracy on test set: 73.940 %\n",
      "epoch: 45 loss:20.198  Accuracy on test set: 77.000 %\n",
      "epoch: 46 loss:18.739  Accuracy on test set: 77.040 %\n",
      "epoch: 47 loss:19.385  Accuracy on test set: 77.560 %\n",
      "epoch: 48 loss:17.623  Accuracy on test set: 76.480 %\n",
      "epoch: 49 loss:19.824  Accuracy on test set: 78.660 %\n",
      "epoch: 50 loss:21.533  Accuracy on test set: 77.200 %\n",
      "epoch: 51 loss:19.192  Accuracy on test set: 76.900 %\n",
      "epoch: 52 loss:27.567  Accuracy on test set: 76.140 %\n",
      "epoch: 53 loss:25.607  Accuracy on test set: 76.680 %\n",
      "epoch: 54 loss:17.992  Accuracy on test set: 77.100 %\n",
      "epoch: 55 loss:16.503  Accuracy on test set: 78.380 %\n",
      "epoch: 56 loss:15.290  Accuracy on test set: 80.140 %\n",
      "epoch: 57 loss:16.325  Accuracy on test set: 80.180 %\n",
      "epoch: 58 loss:14.357  Accuracy on test set: 79.440 %\n",
      "epoch: 59 loss:14.767  Accuracy on test set: 80.940 %\n",
      "epoch: 60 loss:12.349  Accuracy on test set: 79.340 %\n",
      "epoch: 61 loss:14.790  Accuracy on test set: 81.220 %\n",
      "epoch: 62 loss:10.780  Accuracy on test set: 80.460 %\n",
      "epoch: 63 loss:10.038  Accuracy on test set: 81.260 %\n",
      "epoch: 64 loss:10.947  Accuracy on test set: 80.500 %\n",
      "epoch: 65 loss:14.522  Accuracy on test set: 80.900 %\n",
      "epoch: 66 loss:11.505  Accuracy on test set: 81.260 %\n",
      "epoch: 67 loss:11.642  Accuracy on test set: 79.800 %\n",
      "epoch: 68 loss:9.781  Accuracy on test set: 80.820 %\n",
      "epoch: 69 loss:9.335  Accuracy on test set: 80.320 %\n",
      "epoch: 70 loss:11.932  Accuracy on test set: 81.080 %\n",
      "epoch: 71 loss:13.515  Accuracy on test set: 80.580 %\n",
      "epoch: 72 loss:11.690  Accuracy on test set: 81.000 %\n",
      "epoch: 73 loss:8.246  Accuracy on test set: 81.880 %\n",
      "epoch: 74 loss:7.356  Accuracy on test set: 81.960 %\n",
      "epoch: 75 loss:6.474  Accuracy on test set: 81.620 %\n",
      "epoch: 76 loss:6.508  Accuracy on test set: 81.780 %\n",
      "epoch: 77 loss:6.497  Accuracy on test set: 82.000 %\n",
      "epoch: 78 loss:6.490  Accuracy on test set: 82.140 %\n",
      "epoch: 79 loss:8.575  Accuracy on test set: 81.020 %\n",
      "epoch: 80 loss:8.004  Accuracy on test set: 82.080 %\n",
      "epoch: 81 loss:5.833  Accuracy on test set: 81.780 %\n",
      "epoch: 82 loss:5.149  Accuracy on test set: 82.160 %\n",
      "epoch: 83 loss:5.107  Accuracy on test set: 82.060 %\n",
      "epoch: 84 loss:4.989  Accuracy on test set: 82.040 %\n",
      "epoch: 85 loss:4.973  Accuracy on test set: 81.680 %\n",
      "epoch: 86 loss:4.889  Accuracy on test set: 81.780 %\n",
      "epoch: 87 loss:5.147  Accuracy on test set: 81.760 %\n",
      "epoch: 88 loss:4.190  Accuracy on test set: 81.740 %\n",
      "epoch: 89 loss:3.733  Accuracy on test set: 82.140 %\n",
      "epoch: 90 loss:3.740  Accuracy on test set: 82.060 %\n",
      "epoch: 91 loss:3.620  Accuracy on test set: 81.820 %\n",
      "epoch: 92 loss:3.534  Accuracy on test set: 82.060 %\n",
      "epoch: 93 loss:3.520  Accuracy on test set: 82.320 %\n",
      "epoch: 94 loss:3.799  Accuracy on test set: 81.840 %\n",
      "epoch: 95 loss:3.965  Accuracy on test set: 82.200 %\n",
      "epoch: 96 loss:4.105  Accuracy on test set: 82.340 %\n",
      "epoch: 97 loss:3.467  Accuracy on test set: 82.320 %\n",
      "epoch: 98 loss:3.023  Accuracy on test set: 82.260 %\n",
      "epoch: 99 loss:3.266  Accuracy on test set: 82.140 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    epoch=100\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "    criterion = torch.nn.CrossEntropyLoss()# give y hat and y calculate the loss\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)\n",
    "    for i in range(epoch):\n",
    "        train(i)\n",
    "        scheduler.step()\n",
    "        test(test_loader,model,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at: ./model_CIFAR10/model_org.pt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "save_dir = './model_CIFAR10'  # 目录\n",
    "save_path = f'{save_dir}/model_org.pt'  # 文件路径\n",
    "\n",
    "# 确保目录存在，但不要创建文件\n",
    "os.makedirs(save_dir, exist_ok=True)  # ✅ 只创建目录，不影响文件\n",
    "\n",
    "# 保存模型\n",
    "torch.save(model.state_dict(), save_path)  # ✅ 保存文件\n",
    "print(f\"Model saved at: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/chunjie/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n",
      "/tmp/ipykernel_1023834/3949220174.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_test.load_state_dict(torch.load(f'./model_CIFAR10/model_org.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 84.740 %\n"
     ]
    }
   ],
   "source": [
    "model_test=torch.hub.load(\"chenyaofo/pytorch-cifar-models\", \"cifar10_mobilenetv2_x0_5\", pretrained=False)\n",
    "model_test.to(device)\n",
    "model_test.load_state_dict(torch.load(f'./model_CIFAR10/model_org.pt'))\n",
    "model.eval()\n",
    "test(test_loader,model,device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
