{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish data loading\n",
      "Training data size: 5000\n",
      "Testing data size: 5000\n",
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import argparse\n",
    "from denoising_diffusion_pytorch import Unet, GaussianDiffusion, Trainer\n",
    "from torch.optim import Adam,AdamW\n",
    "from tqdm.auto import tqdm\n",
    "import torchvision\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "bt_size=128\n",
    "path = \"./CIFAR10/generated_images\"\n",
    " \n",
    "train_set = datasets.ImageFolder(path, transform=transform_train)\n",
    " \n",
    "train_loader = DataLoader(train_set, batch_size=bt_size, shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "selected_classes = [0, 1, 2, 3, 4]\n",
    "label_mapping = {orig_label: idx for idx, orig_label in enumerate(selected_classes)}\n",
    "\n",
    "test_set = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=False, transform=transform_test\n",
    ")\n",
    "\n",
    "\n",
    "selected_indices_test = [\n",
    "    idx for idx, (_, label) in enumerate(test_set)\n",
    "    if label in selected_classes\n",
    "]\n",
    "\n",
    "\n",
    "for i in selected_indices_test:\n",
    "    test_set.targets[i]=label_mapping[test_set.targets[i]]\n",
    "\n",
    "# 创建子集数据集\n",
    "filtered_test_set = Subset(test_set, selected_indices_test)\n",
    "test_loader = DataLoader(filtered_test_set, batch_size=bt_size, shuffle=False, num_workers=2)\n",
    "\n",
    "print('Finish data loading')\n",
    "print(f\"Training data size: {len(train_set)}\")\n",
    "print(f\"Testing data size: {len(filtered_test_set)}\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/chunjie/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = torch.hub.load(\"chenyaofo/pytorch-cifar-models\", \"cifar10_mobilenetv2_x0_5\", pretrained=False)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()# give y hat and y calculate the loss\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_loader,model,device='cpu'):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:#包含batch size=64张图片和label\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, dim=1)#predicted是一个tensor,torch.tensor([1, 2, 3, ..., 64])  # 共 64 个元素\n",
    "            # 返回最大值和下标\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print('Accuracy on test set: %.3f %%' % (100 * correct / total))\n",
    "\n",
    "def train(epoch):\n",
    "    running_loss = 0.0 \n",
    "\n",
    "    \n",
    "    for batch_idx, data in enumerate(train_loader, 0):\n",
    "\n",
    "        inputs, target = data\n",
    "        inputs, target = inputs.to(device) , target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        #print(batch_idx)\n",
    "\n",
    "        # forward + backward + update\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    print('epoch: %d loss:%.3f ' % (epoch,running_loss), end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss:159.172  Accuracy on test set: 33.440 %\n",
      "epoch: 1 loss:59.465  Accuracy on test set: 41.720 %\n",
      "epoch: 2 loss:48.843  Accuracy on test set: 43.100 %\n",
      "epoch: 3 loss:44.655  Accuracy on test set: 47.800 %\n",
      "epoch: 4 loss:43.382  Accuracy on test set: 48.900 %\n",
      "epoch: 5 loss:40.535  Accuracy on test set: 52.220 %\n",
      "epoch: 6 loss:38.553  Accuracy on test set: 53.920 %\n",
      "epoch: 7 loss:38.380  Accuracy on test set: 52.560 %\n",
      "epoch: 8 loss:37.534  Accuracy on test set: 54.700 %\n",
      "epoch: 9 loss:37.588  Accuracy on test set: 56.040 %\n",
      "epoch: 10 loss:36.176  Accuracy on test set: 56.880 %\n",
      "epoch: 11 loss:36.266  Accuracy on test set: 57.260 %\n",
      "epoch: 12 loss:33.876  Accuracy on test set: 58.380 %\n",
      "epoch: 13 loss:32.780  Accuracy on test set: 60.140 %\n",
      "epoch: 14 loss:34.098  Accuracy on test set: 59.240 %\n",
      "epoch: 15 loss:32.820  Accuracy on test set: 59.420 %\n",
      "epoch: 16 loss:31.443  Accuracy on test set: 59.940 %\n",
      "epoch: 17 loss:28.738  Accuracy on test set: 61.960 %\n",
      "epoch: 18 loss:28.734  Accuracy on test set: 63.620 %\n",
      "epoch: 19 loss:29.724  Accuracy on test set: 60.900 %\n",
      "epoch: 20 loss:29.210  Accuracy on test set: 62.380 %\n",
      "epoch: 21 loss:26.704  Accuracy on test set: 65.220 %\n",
      "epoch: 22 loss:25.615  Accuracy on test set: 65.760 %\n",
      "epoch: 23 loss:26.174  Accuracy on test set: 65.120 %\n",
      "epoch: 24 loss:25.954  Accuracy on test set: 65.060 %\n",
      "epoch: 25 loss:24.758  Accuracy on test set: 69.140 %\n",
      "epoch: 26 loss:22.929  Accuracy on test set: 67.140 %\n",
      "epoch: 27 loss:22.641  Accuracy on test set: 68.840 %\n",
      "epoch: 28 loss:21.052  Accuracy on test set: 69.580 %\n",
      "epoch: 29 loss:19.126  Accuracy on test set: 69.760 %\n",
      "epoch: 30 loss:18.988  Accuracy on test set: 71.580 %\n",
      "epoch: 31 loss:18.055  Accuracy on test set: 70.500 %\n",
      "epoch: 32 loss:18.515  Accuracy on test set: 71.480 %\n",
      "epoch: 33 loss:18.832  Accuracy on test set: 71.480 %\n",
      "epoch: 34 loss:16.015  Accuracy on test set: 72.640 %\n",
      "epoch: 35 loss:16.312  Accuracy on test set: 71.320 %\n",
      "epoch: 36 loss:17.187  Accuracy on test set: 72.880 %\n",
      "epoch: 37 loss:15.877  Accuracy on test set: 72.840 %\n",
      "epoch: 38 loss:13.297  Accuracy on test set: 74.680 %\n",
      "epoch: 39 loss:13.359  Accuracy on test set: 74.100 %\n",
      "epoch: 40 loss:14.918  Accuracy on test set: 73.980 %\n",
      "epoch: 41 loss:15.952  Accuracy on test set: 73.920 %\n",
      "epoch: 42 loss:13.758  Accuracy on test set: 75.440 %\n",
      "epoch: 43 loss:13.197  Accuracy on test set: 74.680 %\n",
      "epoch: 44 loss:11.684  Accuracy on test set: 73.840 %\n",
      "epoch: 45 loss:12.893  Accuracy on test set: 74.980 %\n",
      "epoch: 46 loss:10.800  Accuracy on test set: 76.320 %\n",
      "epoch: 47 loss:12.437  Accuracy on test set: 76.460 %\n",
      "epoch: 48 loss:15.655  Accuracy on test set: 75.520 %\n",
      "epoch: 49 loss:11.350  Accuracy on test set: 75.980 %\n",
      "epoch: 50 loss:12.032  Accuracy on test set: 76.220 %\n",
      "epoch: 51 loss:11.258  Accuracy on test set: 75.860 %\n",
      "epoch: 52 loss:10.753  Accuracy on test set: 77.560 %\n",
      "epoch: 53 loss:10.756  Accuracy on test set: 75.760 %\n",
      "epoch: 54 loss:11.157  Accuracy on test set: 77.020 %\n",
      "epoch: 55 loss:10.537  Accuracy on test set: 77.500 %\n",
      "epoch: 56 loss:8.397  Accuracy on test set: 77.560 %\n",
      "epoch: 57 loss:11.385  Accuracy on test set: 77.260 %\n",
      "epoch: 58 loss:9.144  Accuracy on test set: 77.900 %\n",
      "epoch: 59 loss:8.716  Accuracy on test set: 78.180 %\n",
      "epoch: 60 loss:12.494  Accuracy on test set: 76.680 %\n",
      "epoch: 61 loss:8.692  Accuracy on test set: 78.480 %\n",
      "epoch: 62 loss:7.269  Accuracy on test set: 79.180 %\n",
      "epoch: 63 loss:6.288  Accuracy on test set: 79.000 %\n",
      "epoch: 64 loss:7.278  Accuracy on test set: 79.160 %\n",
      "epoch: 65 loss:6.603  Accuracy on test set: 79.520 %\n",
      "epoch: 66 loss:7.490  Accuracy on test set: 79.100 %\n",
      "epoch: 67 loss:7.265  Accuracy on test set: 78.380 %\n",
      "epoch: 68 loss:7.319  Accuracy on test set: 79.740 %\n",
      "epoch: 69 loss:4.919  Accuracy on test set: 80.520 %\n",
      "epoch: 70 loss:4.244  Accuracy on test set: 79.600 %\n",
      "epoch: 71 loss:4.877  Accuracy on test set: 80.740 %\n",
      "epoch: 72 loss:5.901  Accuracy on test set: 78.780 %\n",
      "epoch: 73 loss:5.263  Accuracy on test set: 80.120 %\n",
      "epoch: 74 loss:5.295  Accuracy on test set: 80.100 %\n",
      "epoch: 75 loss:3.483  Accuracy on test set: 81.020 %\n",
      "epoch: 76 loss:4.416  Accuracy on test set: 79.940 %\n",
      "epoch: 77 loss:5.535  Accuracy on test set: 79.160 %\n",
      "epoch: 78 loss:6.062  Accuracy on test set: 79.820 %\n",
      "epoch: 79 loss:4.363  Accuracy on test set: 80.660 %\n",
      "epoch: 80 loss:3.321  Accuracy on test set: 80.560 %\n",
      "epoch: 81 loss:3.766  Accuracy on test set: 80.640 %\n",
      "epoch: 82 loss:4.138  Accuracy on test set: 80.800 %\n",
      "epoch: 83 loss:3.025  Accuracy on test set: 80.520 %\n",
      "epoch: 84 loss:2.306  Accuracy on test set: 81.000 %\n",
      "epoch: 85 loss:2.302  Accuracy on test set: 80.840 %\n",
      "epoch: 86 loss:2.400  Accuracy on test set: 81.040 %\n",
      "epoch: 87 loss:2.523  Accuracy on test set: 80.920 %\n",
      "epoch: 88 loss:2.194  Accuracy on test set: 80.880 %\n",
      "epoch: 89 loss:1.889  Accuracy on test set: 81.000 %\n",
      "epoch: 90 loss:1.964  Accuracy on test set: 81.100 %\n",
      "epoch: 91 loss:2.615  Accuracy on test set: 81.040 %\n",
      "epoch: 92 loss:1.733  Accuracy on test set: 81.020 %\n",
      "epoch: 93 loss:1.694  Accuracy on test set: 81.220 %\n",
      "epoch: 94 loss:1.548  Accuracy on test set: 81.220 %\n",
      "epoch: 95 loss:1.790  Accuracy on test set: 81.120 %\n",
      "epoch: 96 loss:1.765  Accuracy on test set: 81.100 %\n",
      "epoch: 97 loss:1.578  Accuracy on test set: 81.200 %\n",
      "epoch: 98 loss:1.775  Accuracy on test set: 81.260 %\n",
      "epoch: 99 loss:1.969  Accuracy on test set: 81.180 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    epoch=100\n",
    "\n",
    "    for i in range(epoch):\n",
    "        train(i)\n",
    "        scheduler.step()\n",
    "        test(test_loader,model,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 81.420 %\n",
      "Accuracy on test set: 97.320 %\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test(test_loader,model,device)\n",
    "test(train_loader,model,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at: ./model_CIFAR10/model_gen.pt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "save_dir = './model_CIFAR10'  # 目录\n",
    "save_path = f'{save_dir}/model_gen.pt'  # 文件路径\n",
    "\n",
    "# 确保目录存在，但不要创建文件\n",
    "os.makedirs(save_dir, exist_ok=True)  # ✅ 只创建目录，不影响文件\n",
    "\n",
    "# 保存模型\n",
    "torch.save(model.state_dict(), save_path)  # ✅ 保存文件\n",
    "print(f\"Model saved at: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/chunjie/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n",
      "/tmp/ipykernel_1049276/3687466530.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_test.load_state_dict(torch.load(f'./model_CIFAR10/model_gen.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 81.420 %\n",
      "Accuracy on test set: 97.040 %\n"
     ]
    }
   ],
   "source": [
    "model_test=torch.hub.load(\"chenyaofo/pytorch-cifar-models\", \"cifar10_mobilenetv2_x0_5\", pretrained=False)\n",
    "model_test.to(device)\n",
    "model_test.load_state_dict(torch.load(f'./model_CIFAR10/model_gen.pt'))\n",
    "model_test.eval()\n",
    "test(test_loader,model_test,device)\n",
    "test(train_loader,model_test,device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
